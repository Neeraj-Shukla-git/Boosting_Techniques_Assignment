{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting Techniques | Assignment\n",
        "\n",
        "## Assignment Code: DA-AG-015\n",
        "\n",
        "### Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "- Boosting is an ensemble machine learning technique that combines multiple weak learners (models that perform slightly better than random guessing) into a strong learner with high predictive accuracy.\n",
        "- A weak learner could be a shallow decision tree (often called a decision stump) that may only perform slightly better than chance.\n",
        "- Boosting trains these learners sequentially, where each new model focuses on correcting the mistakes of the previous ones.\n",
        "- How Boosting Improves Weak Learners:\n",
        " - Weighted Training: Misclassified data points from earlier models are given higher weights so the next model learns them better.\n",
        " - Sequential Learning: Each learner builds upon the errors of the previous learners.\n",
        " - Error Reduction: The ensemble gradually reduces bias and variance.\n",
        " - Final Prediction: Combines all weak learners' predictions using weighted voting (classification) or averaging (regression).\n",
        "\n",
        "- Example Analogy:\n",
        "  - Think of boosting like a teacher giving extra attention to students who failed the first test—each new test focuses more on their weak areas until most students pass.\n",
        "\n",
        "### Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "| Feature                 | AdaBoost                                                                            | Gradient Boosting                                                                         |\n",
        "| ----------------------- | ----------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |\n",
        "| **Core Idea**           | Focuses on **misclassified samples** by adjusting their weights for the next model. | Fits the new model to the **residual errors** (negative gradients) of the previous model. |\n",
        "| **Model Sequence**      | Each new model is trained with adjusted **sample weights**.                         | Each new model is trained on **prediction errors** of previous models.                    |\n",
        "| **Loss Function**       | Primarily designed for **exponential loss** (but variants exist).                   | Can optimize **any differentiable loss function** (MSE, log-loss, etc.).                  |\n",
        "| **Weighting**           | Assigns weights to samples and models based on accuracy.                            | Models are combined by adding predictions scaled by the learning rate.                    |\n",
        "| **Robustness to Noise** | More sensitive to noisy data.                                                       | More robust due to flexibility in loss function.                                          |\n",
        "\n",
        "\n",
        "### Question 3: How does regularization help in XGBoost?\n",
        "- Regularization in XGBoost helps prevent overfitting by controlling model complexity.\n",
        "- It uses:\n",
        " - 1. L1 (Lasso) - Encourages sparsity in features.\n",
        " - 2. L2 (Ridge) - Penalizes large weights.\n",
        " - 3. Tree complexity control - Parameters like max_depth, min_child_weight, gamma reduce overfitting.\n",
        "\n",
        "### Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "- Automatic Handling - No need for manual encoding like one-hot or label encoding.\n",
        "- Ordered Target Statistics - Uses special encoding to prevent target leakage.\n",
        "- Efficient GPU Training - Faster than many other boosting libraries.\n",
        "- Less Parameter Tuning - Works well with default parameters.\n",
        "\n",
        "### Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "#### Datasets:\n",
        "● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "\n",
        "● Use sklearn.datasets.fetch_california_housing() for regression\n",
        "tasks.\n",
        "- Fraud Detection (financial transactions → small false negative rate important)\n",
        "- Credit Risk Modeling\n",
        "- Customer Churn Prediction\n",
        "- Medical Diagnosis (e.g., breast cancer detection)\n",
        "- Click-through Rate Prediction in ads\n",
        "- Boosting is preferred when:\n",
        "  - Accuracy is more important than speed.\n",
        "  - The problem is imbalanced.\n",
        "  - There's a lot of noise and non-linear relationships.\n",
        "\n",
        "### Question 6: Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Print the model accuracy\n",
        "\n",
        "### Question 7: Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "● Evaluate performance using R-squared score\n",
        "\n",
        "### Question 8: Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Tune the learning rate using GridSearchCV\n",
        "\n",
        "● Print the best parameters and accuracy\n",
        "\n",
        "### Question 9: Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "\n",
        "● Plot the confusion matrix using seaborn\n",
        "\n",
        "### Question 10: You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior. The dataset is imbalanced, contains missing values, and has both numeric and categorical features. Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "● Hyperparameter tuning strategy\n",
        "\n",
        "● Evaluation metrics you'd choose and why\n",
        "\n",
        "● How the business would benefit from your model\n",
        "\n",
        "- Step-by-step:\n",
        "\n",
        "  - 1. Data Preprocessing\n",
        "    - Handle missing values: median for numeric, mode for categorical.\n",
        "    - Encode categorical:\n",
        "      - CatBoost → no manual encoding needed\n",
        "      - XGBoost/AdaBoost → One-hot or Target encoding\n",
        "      - Scale numerical features (optional for tree-based methods).\n",
        "   - 2. Model Choice\n",
        "     - CatBoost → Best for mixed data types and less preprocessing.\n",
        "     - XGBoost → Highly optimized, great for large datasets.\n",
        "     - AdaBoost → Simple, but less efficient with many categorical variables.\n",
        "   - 3. Hyperparameter Tuning\n",
        "     - Use GridSearchCV or RandomizedSearchCV for learning rate, max depth, n_estimators.\n",
        "   - 4. Evaluation Metrics\n",
        "     - Use F1-score or ROC-AUC for imbalanced data (accuracy is misleading).\n",
        "     - Confusion matrix to visualize false negatives (critical in loan defaults).\n",
        "    - 5. Business Benefit\n",
        "      - Reduce financial losses by identifying risky borrowers.\n",
        "      - Adjust loan approval criteria dynamically.\n",
        "      - Improve customer targeting for safe lending."
      ],
      "metadata": {
        "id": "VfOLmT_LYVWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGrHcsYiYTUR",
        "outputId": "f5d9c555-b04c-451a-eb77-af7a7025caa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Accuracy: 0.9649122807017544\n"
          ]
        }
      ],
      "source": [
        "# Solution 06\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost\n",
        "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict & Accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 7\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load data\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict & R2 score\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Gradient Boosting R² Score:\", r2_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSm2qBYhiKdL",
        "outputId": "23345f5d-dd8f-4f5b-ab49-babf0bb13491"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting R² Score: 0.7756446042829697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 08\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define model\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# GridSearch\n",
        "param_grid = {'learning_rate': [0.01, 0.1, 0.2]}\n",
        "grid = GridSearchCV(xgb, param_grid, cv=3, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "y_pred = grid.predict(X_test)\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buukig-LiOqA",
        "outputId": "36d4e3a6-39fd-46d6-e81c-7adb03f899e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:49:41] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:49:41] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:49:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:49:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:49:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:49:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:49:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:49:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:49:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:49:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'learning_rate': 0.1}\n",
            "XGBoost Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 09\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost (no manual encoding needed)\n",
        "model = CatBoostClassifier(iterations=100, verbose=0, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Confusion Matrix\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('CatBoost Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUeA067tiUGL"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}